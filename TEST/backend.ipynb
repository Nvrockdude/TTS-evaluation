{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription: this is a very sad drawing\n"
     ]
    }
   ],
   "source": [
    "#to convert audio to text\n",
    "import speech_recognition as sr\n",
    "\n",
    "def transcribe_audio(mp3_file):\n",
    "    # Initialize the recognizer\n",
    "    r = sr.Recognizer()\n",
    "\n",
    "    # Load the audio file\n",
    "    with sr.AudioFile(mp3_file) as source:\n",
    "        audio = r.record(source)\n",
    "\n",
    "    # Perform speech recognition\n",
    "    try:\n",
    "        transcription = r.recognize_google(audio)\n",
    "        return transcription\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"Speech recognition could not understand audio\")\n",
    "    except sr.RequestError as e:\n",
    "        print(\"Could not request results from speech recognition service; {0}\".format(e))\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "# Example usage\n",
    "mp3_file = \"output.mp3\"\n",
    "transcription = transcribe_audio(mp3_file)\n",
    "print(\"Transcription:\", transcription)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted emotion: Sad\n",
      "Accuracy: 0.9937007874015747\n",
      "Precision: 0.9936991049195774\n",
      "Recall: 0.9937007874015747\n",
      "F1-Score: 0.9936895763446699\n"
     ]
    }
   ],
   "source": [
    "#emotion prediction for tts\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the CSV data\n",
    "df = pd.read_csv('emotions.csv')\n",
    "\n",
    "# Replace missing values with an empty string\n",
    "df['Text'].fillna('', inplace=True)\n",
    "\n",
    "# Extract input features (X) and labels (y) from the CSV\n",
    "X = df['Text'].values\n",
    "y = df['Emotion'].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorize the text data using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "# Train a model on the training data\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Make predictions on new text inputs\n",
    "\n",
    "text_vectorized = vectorizer.transform([transcription])\n",
    "predicted_emotion = model.predict(text_vectorized)\n",
    "print(\"Predicted emotion:\", predicted_emotion[0])\n",
    "\n",
    "# Evaluate the model on the testing set\n",
    "y_pred = model.predict(X_test_vectorized)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print('Accuracy:', accuracy)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "print('F1-Score:', f1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 6s 6s/step\n",
      "[[0.131831   0.10057315 0.2651936  0.29380453 0.20859776]]\n",
      " \n",
      "The provided audio's naturalness output is:  Natural\n",
      "Predicted emotion: Sad\n",
      "The naturalness of the provided audio is authentic and good, the predicted emotion is Sad.\n"
     ]
    }
   ],
   "source": [
    "#Quality Output\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import librosa\n",
    "import pyttsx3\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def evaluate_text_to_speech_system(text):\n",
    "    # Initialize the pyttsx3 engine\n",
    "    engine = pyttsx3.init()\n",
    "\n",
    "    # Set the rate and volume of the speech\n",
    "    engine.setProperty('rate', 150)\n",
    "    engine.setProperty('volume', 1.0)\n",
    "\n",
    "    # Convert text to speech\n",
    "    engine.say(text)\n",
    "    engine.runAndWait()\n",
    "\n",
    "    \"\"\"  # Get MOS score from user input\n",
    "    mos_score = int(input(\"Please rate the audio quality on a scale of 1 to 5: \"))\n",
    "\n",
    "    return mos_score \"\"\"\n",
    "    return \n",
    "\n",
    "\n",
    "# Load the model\n",
    "loaded_model = load_model(\"E:/Samsung-Test/models/model.h5\")\n",
    "\n",
    "# Load the necessary variables\n",
    "max_length = 99840  # Max sequence length\n",
    "num_mfcc = 8  # Number of MFCC coefficients\n",
    "\n",
    "# Load and preprocess the new audio sample\n",
    "new_audio_path = 'E:\\Samsung\\output.mp3'\n",
    "new_audio, sr = librosa.load(new_audio_path, sr=None, mono=True)\n",
    "new_audio = librosa.resample(new_audio, orig_sr=sr, target_sr=16000)\n",
    "\n",
    "# Extract MFCC features for the new audio sample\n",
    "mfcc = librosa.feature.mfcc(y=new_audio, sr=16000, n_mfcc=num_mfcc)\n",
    "\n",
    "# Pad or truncate the MFCC features to match the expected shape\n",
    "if mfcc.shape[1] < max_length:\n",
    "    mfcc = pad_sequences([mfcc.T], padding='post', maxlen=max_length, dtype='float32').T\n",
    "elif mfcc.shape[1] > max_length:\n",
    "    mfcc = mfcc[:, :max_length]\n",
    "\n",
    "# Reshape the MFCC features to match the model's input shape\n",
    "mfcc = np.expand_dims(mfcc, axis=0)\n",
    "mfcc = np.swapaxes(mfcc, 1, 2)  # Swap axes to match the expected shape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Perform prediction\n",
    "predictions = loaded_model.predict(mfcc)\n",
    "print(predictions)\n",
    "\n",
    "class_names = ['Very Unnatural', 'Unnatural', 'Neutral', 'Natural', 'Completely Natural']\n",
    "predicted_class_index = np.argmax(predictions[0])\n",
    "predicted_class_name = class_names[predicted_class_index]\n",
    "\n",
    "print(\" \")\n",
    "print(\"The provided audio's naturalness output is: \",predicted_class_name)\n",
    "print(\"Predicted emotion:\", predicted_emotion[0])\n",
    "\n",
    "\n",
    "if (predicted_class_name, predicted_emotion[0]) == ('Very Unnatural', 'Excited'):\n",
    "    text = \"The naturalness of the provided audio is unauthentic and bad, the predicted emotion is filled with joy and excitement.\"\n",
    "elif (predicted_class_name, predicted_emotion[0]) == ('Unnatural', 'Excited'):\n",
    "    text = \"The naturalness of the provided audio is unauthentic and ok, the predicted emotion is filled with joy and excitement.\"\n",
    "elif (predicted_class_name, predicted_emotion[0]) == ('Neutral', 'Excited'):\n",
    "    text = \"The naturalness of the provided audio is not great, the predicted emotion is filled with joy and excitement.\"\n",
    "elif (predicted_class_name, predicted_emotion[0]) == ('Natural', 'Excited'):\n",
    "    text = \"The naturalness of the provided audio is authentic and good, the predicted emotion is filled with joy and excitement.\"\n",
    "elif (predicted_class_name, predicted_emotion[0]) == ('Completely Natural', 'Excited'):\n",
    "    text = \"The naturalness of the provided audio is authentic and very natural, the predicted emotion is filled with joy and excitement.\"\n",
    "elif (predicted_class_name, predicted_emotion[0]) == ('Very Unnatural', 'Happy'):\n",
    "    text = \"The naturalness of the provided audio is unauthentic and bad, the predicted emotion is happy.\"\n",
    "elif (predicted_class_name, predicted_emotion[0]) == ('Unnatural', 'Happy'):\n",
    "    text = \"The naturalness of the provided audio is unauthentic and ok, the predicted emotion is happy.\"\n",
    "elif (predicted_class_name, predicted_emotion[0]) == ('Neutral', 'Happy'):\n",
    "    text = \"The naturalness of the provided audio is not great, the predicted emotion is happy.\"\n",
    "elif (predicted_class_name, predicted_emotion[0]) == ('Natural', 'Happy'):\n",
    "    text = \"The naturalness of the provided audio is authentic and good, the predicted emotion is happy.\"\n",
    "elif (predicted_class_name, predicted_emotion[0]) == ('Completely Natural', 'Happy'):\n",
    "    text = \"The naturalness of the provided audio is authentic and very natural, the predicted emotion is happy.\"\n",
    "elif (predicted_class_name, predicted_emotion[0]) == ('Very Unnatural', 'Anger'):\n",
    "    text = \"The naturalness of the provided audio is unauthentic and bad, the predicted emotion is anger.\"\n",
    "elif (predicted_class_name, predicted_emotion[0]) == ('Unnatural', 'Anger'):\n",
    "    text = \"The naturalness of the provided audio is unauthentic and ok, the predicted emotion is anger.\"\n",
    "elif (predicted_class_name, predicted_emotion[0]) == ('Neutral', 'Anger'):\n",
    "    text = \"The naturalness of the provided audio is not great, the predicted emotion is anger.\"\n",
    "elif (predicted_class_name, predicted_emotion[0]) == ('Natural', 'Anger'):\n",
    "    text = \"The naturalness of the provided audio is authentic and good, the predicted emotion is anger.\"\n",
    "elif (predicted_class_name, predicted_emotion[0]) == ('Completely Natural', 'Anger'):\n",
    "    text = \"The naturalness of the provided audio is authentic and very natural, the predicted emotion is anger.\"\n",
    "elif (predicted_class_name, predicted_emotion[0]) == ('Very Unnatural', 'Base'):\n",
    "    text = \"The naturalness of the provided audio is unauthentic and bad, the predicted emotion is base.\"\n",
    "elif (predicted_class_name, predicted_emotion[0]) == ('Unnatural', 'Base'):\n",
    "    text = \"The naturalness of the provided audio is unauthentic and ok, the predicted emotion is base.\"\n",
    "elif (predicted_class_name, predicted_emotion[0]) == ('Neutral', 'Base'):\n",
    "    text = \"The naturalness of the provided audio is not great, the predicted emotion is base.\"\n",
    "elif (predicted_class_name, predicted_emotion[0]) == ('Natural', 'Base'):\n",
    "    text = \"The naturalness of the provided audio is authentic and good, the predicted emotion is base.\"\n",
    "elif (predicted_class_name, predicted_emotion[0]) == ('Completely Natural', 'Base'):\n",
    "    text = \"The naturalness of the provided audio is authentic and very natural, the predicted emotion is base.\"\n",
    "elif (predicted_class_name, predicted_emotion[0]) == ('Very Unnatural', 'Calm'):\n",
    "    text = \"The naturalness of the provided audio is unauthentic and bad, the predicted emotion is calm.\"\n",
    "elif (predicted_class_name, predicted_emotion[0]) == ('Unnatural', 'Calm'):\n",
    "    text = \"The naturalness of the provided audio is unauthentic and ok, the predicted emotion is calm.\"\n",
    "elif (predicted_class_name, predicted_emotion[0]) == ('Neutral', 'Calm'):\n",
    "    text = \"The naturalness of the provided audio is not great, the predicted emotion is calm.\"\n",
    "elif (predicted_class_name, predicted_emotion[0]) == ('Natural', 'Calm'):\n",
    "    text = \"The naturalness of the provided audio is authentic and good, the predicted emotion is calm.\"\n",
    "elif (predicted_class_name, predicted_emotion[0]) == ('Completely Natural', 'Calm'):\n",
    "    text = \"The naturalness of the provided audio is authentic and very natural, the predicted emotion is calm.\"\n",
    "elif (predicted_class_name, predicted_emotion[0]) == ('Very Unnatural', 'Surprise'):\n",
    "    text = \"The naturalness of the provided audio is unauthentic and bad, the predicted emotion is surprise.\"\n",
    "elif (predicted_class_name, predicted_emotion[0]) == ('Unnatural', 'Surprise'):\n",
    "    text = \"The naturalness of the provided audio is unauthentic and ok, the predicted emotion is surprise.\"\n",
    "elif (predicted_class_name, predicted_emotion[0]) == ('Neutral', 'Surprise'):\n",
    "    text = \"The naturalness of the provided audio is not great, the predicted emotion is surprise.\"\n",
    "elif (predicted_class_name, predicted_emotion[0]) == ('Natural', 'Surprise'):\n",
    "    text = \"The naturalness of the provided audio is authentic and good, the predicted emotion is surprise.\"\n",
    "elif (predicted_class_name, predicted_emotion[0]) == ('Completely Natural', 'Surprise'):\n",
    "    text = \"The naturalness of the provided audio is authentic and very natural, the predicted emotion is surprise.\"\n",
    "elif (predicted_class_name, predicted_emotion[0]) == ('Very Unnatural', 'Fear'):\n",
    "    text = \"The naturalness of the provided audio is unauthentic and bad, the predicted emotion is fear.\"\n",
    "elif (predicted_class_name, predicted_emotion[0]) == ('Unnatural', 'Fear'):\n",
    "    text = \"The naturalness of the provided audio is unauthentic and ok, the predicted emotion is fear.\"\n",
    "elif (predicted_class_name, predicted_emotion[0]) == ('Neutral', 'Fear'):\n",
    "    text = \"The naturalness of the provided audio is not great, the predicted emotion is fear.\"\n",
    "elif (predicted_class_name, predicted_emotion[0]) == ('Natural', 'Fear'):\n",
    "    text = \"The naturalness of the provided audio is authentic and good, the predicted emotion is fear.\"\n",
    "elif (predicted_class_name, predicted_emotion[0]) == ('Completely Natural', 'Fear'):\n",
    "    text = \"The naturalness of the provided audio is authentic and very natural, the predicted emotion is fear.\"\n",
    "elif (predicted_class_name, predicted_emotion[0]) == ('Very Unnatural', 'Apologetic'):\n",
    "    text = \"The naturalness of the provided audio is unauthentic and bad, the predicted emotion is apologetic.\"\n",
    "elif (predicted_class_name, predicted_emotion[0]) == ('Unnatural', 'Apologetic'):\n",
    "    text = \"The naturalness of the provided audio is unauthentic and ok, the predicted emotion is apologetic.\"\n",
    "elif (predicted_class_name, predicted_emotion[0]) == ('Neutral', 'Apologetic'):\n",
    "    text = \"The naturalness of the provided audio is not great, the predicted emotion is apologetic.\"\n",
    "elif (predicted_class_name, predicted_emotion[0]) == ('Natural', 'Apologetic'):\n",
    "    text = \"The naturalness of the provided audio is authentic and good, the predicted emotion is apologetic.\"\n",
    "elif (predicted_class_name, predicted_emotion[0]) == ('Completely Natural', 'Apologetic'):\n",
    "    text = \"The naturalness of the provided audio is authentic and very natural, the predicted emotion is apologetic.\"\n",
    "elif (predicted_class_name, predicted_emotion[0]) == ('Very Unnatural', 'Sad'):\n",
    "    text = \"The naturalness of the provided audio is unauthentic and bad, the predicted emotion is Sad.\"\n",
    "elif (predicted_class_name, predicted_emotion[0]) == ('Unnatural', 'Sad'):\n",
    "    text = \"The naturalness of the provided audio is unauthentic and ok, the predicted emotion is Sad.\"\n",
    "elif (predicted_class_name, predicted_emotion[0]) == ('Neutral', 'Sad'):\n",
    "    text = \"The naturalness of the provided audio is not great, the predicted emotion is Sad.\"\n",
    "elif (predicted_class_name, predicted_emotion[0]) == ('Natural', 'Sad'):\n",
    "    text = \"The naturalness of the provided audio is authentic and good, the predicted emotion is Sad.\"\n",
    "elif (predicted_class_name, predicted_emotion[0]) == ('Completely Natural', 'Sad'):\n",
    "    text = \"The naturalness of the provided audio is authentic and very natural, the predicted emotion is Sad.\"\n",
    "else:\n",
    "    text = \"No text available for the given combination.\"\n",
    "\n",
    "print (text)\n",
    "evaluate_text_to_speech_system(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n"
     ]
    }
   ],
   "source": [
    "# First Trial\n",
    "from flask import Flask, request, jsonify\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import speech_recognition as sr\n",
    "import os\n",
    "from pydub import AudioSegment\n",
    "\n",
    "\n",
    "# Load the emotion recognition model\n",
    "model = keras.models.load_model('E:\\Samsung-Test\\TEST\\models\\model.h5')\n",
    "\n",
    "# Initialize the Flask app\n",
    "app = Flask(__name__)\n",
    "\n",
    "\n",
    "\"\"\" def transcribe_audio(mp3_file):\n",
    "    wav_file = 'temp_audio.wav'\n",
    "\n",
    "    # Convert MP3 to WAV using pydub\n",
    "    audio = AudioSegment.from_mp3(mp3_file)\n",
    "    audio.export(wav_file, format='wav')\n",
    "    \n",
    "    wav_file = 'temp_audio.wav'\n",
    "    # Transcribe the audio file using SpeechRecognition\n",
    "    r = sr.Recognizer()\n",
    "    with sr.AudioFile(wav_file) as source:\n",
    "        audio_data = r.record(source)\n",
    "        transcription = r.recognize_google(audio_data)\n",
    "\n",
    "    # Remove the temporary WAV file\n",
    "    os.remove(wav_file)\n",
    "\n",
    "    return transcription \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" v11def transcribe_audio(mp3_file):\n",
    "    # Initialize the recognizer\n",
    "    recognizer = sr.Recognizer()\n",
    "\n",
    "    # Load the audio file\n",
    "    with sr.AudioFile(mp3_file) as source:\n",
    "        audio = recognizer.record(source)\n",
    "\n",
    "    # Perform speech recognition\n",
    "    try:\n",
    "        transcription = recognizer.recognize_google(audio)\n",
    "        return transcription\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"Speech recognition could not understand audio\")\n",
    "    except sr.RequestError as e:\n",
    "        print(\"Could not request results from speech recognition service; {0}\".format(e))\n",
    "    return \"\" \"\"\"\n",
    "\n",
    "def transcribe_audio(mp3_file):\n",
    "    # Convert audio file to PCM WAV format using FFmpeg\n",
    "    wav_file = 'temp_audio.wav'\n",
    "    subprocess.run(['ffmpeg', '-i', mp3_file, '-acodec', 'pcm_s16le', '-ar', '16000', '-ac', '1', wav_file], check=True)\n",
    "\n",
    "    # Initialize the recognizer\n",
    "    recognizer = sr.Recognizer()\n",
    "\n",
    "    # Load the audio file\n",
    "    with sr.AudioFile(wav_file) as source:\n",
    "        audio = recognizer.record(source)\n",
    "\n",
    "    # Perform speech recognition\n",
    "    try:\n",
    "        transcription = recognizer.recognize_google(audio)\n",
    "        return transcription\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"Speech recognition could not understand audio\")\n",
    "    except sr.RequestError as e:\n",
    "        print(\"Could not request results from speech recognition service; {0}\".format(e))\n",
    "    finally:\n",
    "        # Clean up the temporary WAV file\n",
    "        os.remove(wav_file)\n",
    "\n",
    "    return \n",
    "\n",
    "\n",
    "def predict_emotion(text):\n",
    "    # Load the CSV data\n",
    "    df = pd.read_csv('E:\\Samsung-Test\\TEST\\emotions.csv')\n",
    "\n",
    "    # Replace missing values with an empty string\n",
    "    df['Text'].fillna('', inplace=True)\n",
    "\n",
    "    # Extract input features (X) and labels (y) from the CSV\n",
    "    X = df['Text'].values\n",
    "    y = df['Emotion'].values\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Vectorize the text data using TF-IDF\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "    X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "    # Train a model on the training data\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "    # Make predictions on new text inputs\n",
    "    text_vectorized = vectorizer.transform([text])\n",
    "    predicted_emotion = model.predict(text_vectorized)\n",
    "    \n",
    "    return predicted_emotion[0]\n",
    "\n",
    "def predict_naturalness(audio_path):\n",
    "    # Load the necessary variables\n",
    "    max_length = 99840  # Max sequence length\n",
    "    num_mfcc = 8  # Number of MFCC coefficients\n",
    "\n",
    "    # Load and preprocess the new audio sample\n",
    "    new_audio, sr = librosa.load(audio_path, sr=None, mono=True)\n",
    "    new_audio = librosa.resample(new_audio, orig_sr=sr, target_sr=16000)\n",
    "\n",
    "    # Extract MFCC features for the new audio sample\n",
    "    mfcc = librosa.feature.mfcc(y=new_audio, sr=16000, n_mfcc=num_mfcc)\n",
    "\n",
    "    # Pad or truncate the MFCC features to match the expected shape\n",
    "    if mfcc.shape[1] < max_length:\n",
    "        mfcc = pad_sequences([mfcc.T], padding='post', maxlen=max_length, dtype='float32').T\n",
    "    elif mfcc.shape[1] > max_length:\n",
    "        mfcc = mfcc[:, :max_length]\n",
    "\n",
    "    # Reshape the MFCC features to match the model's input shape\n",
    "    mfcc = np.expand_dims(mfcc, axis=0)\n",
    "    mfcc = np.swapaxes(mfcc, 1, 2)  # Swap axes to match the expected shape\n",
    "\n",
    "    # Perform prediction\n",
    "    predictions = model.predict(mfcc)\n",
    "    class_names = ['Very Unnatural', 'Unnatural', 'Neutral', 'Natural', 'Completely Natural']\n",
    "    predicted_class_index = np.argmax(predictions[0])\n",
    "    predicted_class_name = class_names[predicted_class_index]\n",
    "\n",
    "    return predicted_class_name\n",
    "\n",
    "@app.route('/process-audio', methods=['POST'])\n",
    "def process_audio():\n",
    "    # Check if the audio file is present in the request\n",
    "    if 'audio' not in request.files:\n",
    "        return jsonify({'error': 'No audio file found'})\n",
    "\n",
    "    audio_file = request.files['audio']\n",
    "\n",
    "    # Save the audio file temporarily\n",
    "    audio_path = 'temp_audio.wav'\n",
    "    audio_file.save(audio_path)\n",
    "\n",
    "    # Perform speech recognition\n",
    "    transcription = transcribe_audio(audio_path)\n",
    "\n",
    "    # Perform emotion recognition\n",
    "    predicted_emotion = predict_emotion(transcription)\n",
    "\n",
    "    # Perform naturalness classification\n",
    "    predicted_naturalness = predict_naturalness(audio_path)\n",
    "\n",
    "    # Clean up the temporary audio file\n",
    "    os.remove(audio_path)\n",
    "\n",
    "    # Return the results as JSON\n",
    "    result = {\n",
    "        'naturalness': predicted_naturalness,\n",
    "        'emotion': predicted_emotion\n",
    "    }\n",
    "\n",
    "    return jsonify(result)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 6s 6s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [16/Jun/2023 17:50:41] \"POST /process-audio HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 4s 4s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [16/Jun/2023 17:52:04] \"POST /process-audio HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 4s 4s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [16/Jun/2023 17:53:26] \"POST /process-audio HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import speech_recognition as sr\n",
    "from pydub import AudioSegment\n",
    "from flask import Flask, request, jsonify\n",
    "import os\n",
    "from pydub import AudioSegment\n",
    "import speech_recognition as sr\n",
    "import librosa\n",
    "\n",
    "# Load the emotion recognition model\n",
    "model = keras.models.load_model('E:\\Samsung-Test\\TEST\\models\\model.h5')\n",
    "\n",
    "\n",
    "# Initialize the Flask app\n",
    "app = Flask(__name__)\n",
    "\n",
    "def transcribe_audio(mp3_file):\n",
    "    # Convert audio file to WAV format using FFmpeg\n",
    "    wav_file = 'temp_audio.wav'\n",
    "    ffmpeg_path = 'ffmpeg'  # Update with the correct path to ffmpeg if necessary\n",
    "    command = [ffmpeg_path, '-i', mp3_file, '-acodec', 'pcm_s16le', '-ar', '16000', '-ac', '1', wav_file]\n",
    "    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    _, error = process.communicate()\n",
    "\n",
    "    if process.returncode != 0:\n",
    "        print(f\"Audio conversion failed: {error.decode('utf-8').strip()}\")\n",
    "        return None\n",
    "\n",
    "    # Initialize the recognizer\n",
    "    recognizer = sr.Recognizer()\n",
    "\n",
    "    # Load the audio file\n",
    "    with sr.AudioFile(wav_file) as source:\n",
    "        audio = recognizer.record(source)\n",
    "\n",
    "    # Perform speech recognition\n",
    "    try:\n",
    "        transcription = recognizer.recognize_google(audio)\n",
    "        return transcription\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"Speech recognition could not understand audio\")\n",
    "    except sr.RequestError as e:\n",
    "        print(\"Could not request results from speech recognition service; {0}\".format(e))\n",
    "    finally:\n",
    "        # Clean up the temporary WAV file\n",
    "        os.remove(wav_file)\n",
    "\n",
    "    return \"Function passed\"\n",
    "\n",
    "\n",
    "\n",
    "def predict_emotion(text):\n",
    "    # Load the CSV data\n",
    "    df = pd.read_csv('E:\\Samsung-Test\\TEST\\emotions.csv')\n",
    "\n",
    "    # Replace missing values with an empty string\n",
    "    df['Text'].fillna('', inplace=True)\n",
    "\n",
    "    # Extract input features (X) and labels (y) from the CSV\n",
    "    X = df['Text'].values\n",
    "    y = df['Emotion'].values\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Vectorize the text data using TF-IDF\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "    X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "    # Train a model on the training data\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "    # Make predictions on new text inputs\n",
    "    text_vectorized = vectorizer.transform([text])\n",
    "    predicted_emotion = model.predict(text_vectorized)\n",
    "\n",
    "    return predicted_emotion[0]\n",
    "\n",
    "\n",
    "def predict_naturalness(audio_path):\n",
    "    # Load the necessary variables\n",
    "    max_length = 99840  # Max sequence length\n",
    "    num_mfcc = 8  # Number of MFCC coefficients\n",
    "\n",
    "    # Load and preprocess the new audio sample\n",
    "    new_audio, sr = librosa.load(audio_path, sr=None, mono=True)\n",
    "    new_audio = librosa.resample(new_audio, orig_sr=sr, target_sr=16000)\n",
    "\n",
    "    # Extract MFCC features for the new audio sample\n",
    "    mfcc = librosa.feature.mfcc(y=new_audio, sr=16000, n_mfcc=num_mfcc)\n",
    "\n",
    "    # Pad or truncate the MFCC features to match the expected shape\n",
    "    if mfcc.shape[1] < max_length:\n",
    "        mfcc = pad_sequences([mfcc.T], padding='post', maxlen=max_length, dtype='float32').T\n",
    "    elif mfcc.shape[1] > max_length:\n",
    "        mfcc = mfcc[:, :max_length]\n",
    "\n",
    "    # Reshape the MFCC features to match the model's input shape\n",
    "    mfcc = np.expand_dims(mfcc, axis=0)\n",
    "    mfcc = np.swapaxes(mfcc, 1, 2)  # Swap axes to match the expected shape\n",
    "\n",
    "    # Perform prediction\n",
    "    predictions = model.predict(mfcc)\n",
    "    class_names = ['Very Unnatural', 'Unnatural', 'Neutral', 'Natural', 'Completely Natural']\n",
    "    predicted_class_index = np.argmax(predictions[0])\n",
    "    predicted_class_name = class_names[predicted_class_index]\n",
    "\n",
    "    return predicted_class_name\n",
    "\n",
    "\n",
    "@app.route('/process-audio', methods=['POST'])\n",
    "def process_audio():\n",
    "    # Check if the audio file is present in the request\n",
    "    if 'audio' not in request.files:\n",
    "        return jsonify({'error': 'No audio file found'})\n",
    "\n",
    "    audio_file = request.files['audio']\n",
    "\n",
    "    # Save the audio file temporarily\n",
    "    audio_path = 'temp_audio.mp3'\n",
    "    audio_file.save(audio_path)\n",
    "\n",
    "    # Perform speech recognition\n",
    "    transcription = transcribe_audio(audio_path)\n",
    "\n",
    "    # Perform emotion recognition\n",
    "    predicted_emotion = predict_emotion(transcription)\n",
    "\n",
    "    # Perform naturalness classification\n",
    "    predicted_naturalness = predict_naturalness(audio_path)\n",
    "\n",
    "    # Clean up the temporary audio file\n",
    "    os.remove(audio_path)\n",
    "\n",
    "    # Return the results as JSON\n",
    "    result = {\n",
    "        'naturalness': predicted_naturalness,\n",
    "        'emotion': predicted_emotion\n",
    "    }\n",
    "\n",
    "    return jsonify(result)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 8s 8s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [16/Jun/2023 18:08:55] \"POST /process-audio HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "# uding the ffmpeg udating the code\n",
    "#Latest half working code\n",
    "\n",
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import speech_recognition as sr\n",
    "from pydub import AudioSegment\n",
    "import librosa\n",
    "\n",
    "# Load the emotion recognition model\n",
    "model = keras.models.load_model('E:\\Samsung-Test\\TEST\\models\\model.h5')\n",
    "\n",
    "# Initialize the Flask app\n",
    "app = Flask(__name__)\n",
    "CORS(app)  # Enable CORS\n",
    "\n",
    "def transcribe_audio(mp3_file):\n",
    "    # Convert audio file to WAV format using FFmpeg\n",
    "    wav_file = 'temp_audio.wav'\n",
    "    ffmpeg_path = 'ffmpeg'  # Update with the correct path to ffmpeg if necessary\n",
    "    command = [ffmpeg_path, '-i', mp3_file, '-acodec', 'pcm_s16le', '-ar', '16000', '-ac', '1', wav_file]\n",
    "    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    _, error = process.communicate()\n",
    "\n",
    "    if process.returncode != 0:\n",
    "        print(f\"Audio conversion failed: {error.decode('utf-8').strip()}\")\n",
    "        return None\n",
    "\n",
    "    # Initialize the recognizer\n",
    "    recognizer = sr.Recognizer()\n",
    "\n",
    "    # Load the audio file\n",
    "    with sr.AudioFile(wav_file) as source:\n",
    "        audio = recognizer.record(source)\n",
    "\n",
    "    # Perform speech recognition\n",
    "    try:\n",
    "        transcription = recognizer.recognize_google(audio)\n",
    "        return transcription\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"Speech recognition could not understand audio\")\n",
    "    except sr.RequestError as e:\n",
    "        print(\"Could not request results from speech recognition service; {0}\".format(e))\n",
    "    finally:\n",
    "        # Clean up the temporary WAV file\n",
    "        os.remove(wav_file)\n",
    "\n",
    "    return \"Function passed\"\n",
    "\n",
    "\n",
    "def predict_emotion(text):\n",
    "    # Load the CSV data\n",
    "    df = pd.read_csv('E:\\Samsung-Test\\TEST\\emotions.csv')\n",
    "\n",
    "    # Replace missing values with an empty string\n",
    "    df['Text'].fillna('', inplace=True)\n",
    "\n",
    "    # Extract input features (X) and labels (y) from the CSV\n",
    "    X = df['Text'].values\n",
    "    y = df['Emotion'].values\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Vectorize the text data using TF-IDF\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "    X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "    # Train a model on the training data\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "    # Make predictions on new text inputs\n",
    "    text_vectorized = vectorizer.transform([text])\n",
    "    predicted_emotion = model.predict(text_vectorized)\n",
    "\n",
    "    return predicted_emotion[0]\n",
    "\n",
    "\n",
    "def predict_naturalness(audio_path):\n",
    "    # Load the necessary variables\n",
    "    max_length = 99840  # Max sequence length\n",
    "    num_mfcc = 8  # Number of MFCC coefficients\n",
    "\n",
    "    # Load and preprocess the new audio sample\n",
    "    new_audio, sr = librosa.load(audio_path, sr=None, mono=True)\n",
    "    new_audio = librosa.resample(new_audio, orig_sr=sr, target_sr=16000)\n",
    "\n",
    "    # Extract MFCC features for the new audio sample\n",
    "    mfcc = librosa.feature.mfcc(y=new_audio, sr=16000, n_mfcc=num_mfcc)\n",
    "\n",
    "    # Pad or truncate the MFCC features to match the expected shape\n",
    "    if mfcc.shape[1] < max_length:\n",
    "        mfcc = pad_sequences([mfcc.T], padding='post', maxlen=max_length, dtype='float32').T\n",
    "    elif mfcc.shape[1] > max_length:\n",
    "        mfcc = mfcc[:, :max_length]\n",
    "\n",
    "    # Reshape the MFCC features to match the model's input shape\n",
    "    mfcc = np.expand_dims(mfcc, axis=0)\n",
    "    mfcc = np.swapaxes(mfcc, 1, 2)  # Swap axes to match the expected shape\n",
    "\n",
    "    # Perform prediction\n",
    "    predictions = model.predict(mfcc)\n",
    "    class_names = ['Very Unnatural', 'Unnatural', 'Neutral', 'Natural', 'Completely Natural']\n",
    "    predicted_class_index = np.argmax(predictions[0])\n",
    "    predicted_class_name = class_names[predicted_class_index]\n",
    "\n",
    "    return predicted_class_name\n",
    "\n",
    "\n",
    "@app.route('/process-audio', methods=['POST'])\n",
    "def process_audio():\n",
    "    # Check if the audio file is present in the request\n",
    "    if 'audio' not in request.files:\n",
    "        return jsonify({'error': 'No audio file found'})\n",
    "\n",
    "    audio_file = request.files['audio']\n",
    "\n",
    "    # Save the audio file temporarily\n",
    "    audio_path = 'temp_audio.mp3'\n",
    "    audio_file.save(audio_path)\n",
    "\n",
    "    # Perform speech recognition\n",
    "    transcription = transcribe_audio(audio_path)\n",
    "\n",
    "    # Perform emotion recognition\n",
    "    predicted_emotion = predict_emotion(transcription)\n",
    "\n",
    "    # Perform naturalness classification\n",
    "    predicted_naturalness = predict_naturalness(audio_path)\n",
    "\n",
    "    # Clean up the temporary audio file\n",
    "    os.remove(audio_path)\n",
    "\n",
    "    # Return the results as JSON\n",
    "    result = {\n",
    "        'naturalness': predicted_naturalness,\n",
    "        'emotion': predicted_emotion\n",
    "    }\n",
    "\n",
    "    return jsonify(result)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      " * Restarting with stat\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\neera\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:3516: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "#Using flask updated\n",
    "#Trying to debug\n",
    "\n",
    "from flask import Flask, request, jsonify, render_template\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import speech_recognition as sr\n",
    "from pydub import AudioSegment\n",
    "import librosa\n",
    "\n",
    "# Load the emotion recognition model\n",
    "model = keras.models.load_model('E:\\Samsung-Test\\TEST\\models\\model.h5')\n",
    "\n",
    "\n",
    "# Initialize the Flask app\n",
    "app = Flask(__name__)\n",
    "\n",
    "def transcribe_audio(mp3_file):\n",
    "    # Convert audio file to WAV format using FFmpeg\n",
    "    wav_file = 'temp_audio.wav'\n",
    "    ffmpeg_path = 'ffmpeg'  # Update with the correct path to ffmpeg if necessary\n",
    "    command = [ffmpeg_path, '-i', mp3_file, '-acodec', 'pcm_s16le', '-ar', '16000', '-ac', '1', wav_file]\n",
    "    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    _, error = process.communicate()\n",
    "\n",
    "    if process.returncode != 0:\n",
    "        print(f\"Audio conversion failed: {error.decode('utf-8').strip()}\")\n",
    "        return None\n",
    "\n",
    "    # Initialize the recognizer\n",
    "    recognizer = sr.Recognizer()\n",
    "\n",
    "    # Load the audio file\n",
    "    with sr.AudioFile(wav_file) as source:\n",
    "        audio = recognizer.record(source)\n",
    "\n",
    "    # Perform speech recognition\n",
    "    try:\n",
    "        transcription = recognizer.recognize_google(audio)\n",
    "        return transcription\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"Speech recognition could not understand audio\")\n",
    "    except sr.RequestError as e:\n",
    "        print(\"Could not request results from speech recognition service; {0}\".format(e))\n",
    "    finally:\n",
    "        # Clean up the temporary WAV file\n",
    "        os.remove(wav_file)\n",
    "\n",
    "    return \"Function passed\"\n",
    "\n",
    "\n",
    "\n",
    "def predict_emotion(text):\n",
    "    # Load the CSV data\n",
    "    df = pd.read_csv('E:\\Samsung-Test\\TEST\\emotions.csv')\n",
    "\n",
    "    # Replace missing values with an empty string\n",
    "    df['Text'].fillna('', inplace=True)\n",
    "\n",
    "    # Extract input features (X) and labels (y) from the CSV\n",
    "    X = df['Text'].values\n",
    "    y = df['Emotion'].values\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Vectorize the text data using TF-IDF\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "    X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "    # Train a model on the training data\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "    # Make predictions on new text inputs\n",
    "    text_vectorized = vectorizer.transform([text])\n",
    "    predicted_emotion = model.predict(text_vectorized)\n",
    "\n",
    "    return predicted_emotion[0]\n",
    "\n",
    "\n",
    "def predict_naturalness(audio_path):\n",
    "    # Load the necessary variables\n",
    "    max_length = 99840  # Max sequence length\n",
    "    num_mfcc = 8  # Number of MFCC coefficients\n",
    "\n",
    "    # Load and preprocess the new audio sample\n",
    "    new_audio, sr = librosa.load(audio_path, sr=None, mono=True)\n",
    "    new_audio = librosa.resample(new_audio, orig_sr=sr, target_sr=16000)\n",
    "\n",
    "    # Extract MFCC features for the new audio sample\n",
    "    mfcc = librosa.feature.mfcc(y=new_audio, sr=16000, n_mfcc=num_mfcc)\n",
    "\n",
    "    # Pad or truncate the MFCC features to match the expected shape\n",
    "    if mfcc.shape[1] < max_length:\n",
    "        mfcc = pad_sequences([mfcc.T], padding='post', maxlen=max_length, dtype='float32').T\n",
    "    elif mfcc.shape[1] > max_length:\n",
    "        mfcc = mfcc[:, :max_length]\n",
    "\n",
    "    # Reshape the MFCC features to match the model's input shape\n",
    "    mfcc = np.expand_dims(mfcc, axis=0)\n",
    "    mfcc = np.swapaxes(mfcc, 1, 2)  # Swap axes to match the expected shape\n",
    "\n",
    "    # Perform prediction\n",
    "    predictions = model.predict(mfcc)\n",
    "    class_names = ['Very Unnatural', 'Unnatural', 'Neutral', 'Natural', 'Completely Natural']\n",
    "    predicted_class_index = np.argmax(predictions[0])\n",
    "    predicted_class_name = class_names[predicted_class_index]\n",
    "\n",
    "    return predicted_class_name\n",
    "\n",
    "\n",
    "# UPLOAD_FOLDER = r'E:\\Samsung-Test\\TEST\\backend.ipynb'\n",
    "# app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER\n",
    "\n",
    "@app.route('/home')\n",
    "def index():\n",
    "    return render_template('index.html')\n",
    "\n",
    "\n",
    "@app.route('/process-audio', methods=['POST'])\n",
    "def process_audio():\n",
    "    # Check if the audio file is present in the request\n",
    "    if 'audio' not in request.files:\n",
    "        return jsonify({'error': 'No audio file found'})\n",
    "\n",
    "    audio_file = request.files['audio']\n",
    "\n",
    "    # Save the audio file temporarily\n",
    "    audio_path = 'temp_audio.mp3'\n",
    "    audio_file.save(audio_path)\n",
    "\n",
    "    try:\n",
    "        # Perform speech recognition\n",
    "        transcription = transcribe_audio(audio_path)\n",
    "\n",
    "        # Perform emotion recognition\n",
    "        predicted_emotion = predict_emotion(transcription)\n",
    "\n",
    "        # Perform naturalness classification\n",
    "        predicted_naturalness = predict_naturalness(audio_path)\n",
    "\n",
    "        # Clean up the temporary audio file\n",
    "        os.remove(audio_path)\n",
    "\n",
    "        # Return the results as JSON\n",
    "        result = {\n",
    "            'naturalness': predicted_naturalness,\n",
    "            'emotion': predicted_emotion\n",
    "        }\n",
    "\n",
    "        return jsonify(result)\n",
    "    except Exception as e:\n",
    "        return jsonify({'error': str(e)})\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
