{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'E:/somos/audios/audios'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m ratings_file \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mE:/somos/raw_scores_with_metadata/raw_scores.tsv\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     19\u001b[0m \u001b[39m# Load audio samples\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m audio_files \u001b[39m=\u001b[39m [file \u001b[39mfor\u001b[39;00m file \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39;49mlistdir(audio_folder) \u001b[39mif\u001b[39;00m file\u001b[39m.\u001b[39mendswith(\u001b[39m'\u001b[39m\u001b[39m.wav\u001b[39m\u001b[39m'\u001b[39m)]\n\u001b[0;32m     21\u001b[0m audio_samples \u001b[39m=\u001b[39m []\n\u001b[0;32m     22\u001b[0m audio_ids \u001b[39m=\u001b[39m []\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'E:/somos/audios/audios'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import tensorflow as keras\n",
    "from keras import layers\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Reshape\n",
    "\n",
    "\n",
    "# Load the audio samples and quality ratings\n",
    "audio_folder = 'E:/somos/audios/audios'\n",
    "ratings_file = 'E:/somos/raw_scores_with_metadata/raw_scores.tsv'\n",
    "\n",
    "# Load audio samples\n",
    "audio_files = [file for file in os.listdir(audio_folder) if file.endswith('.wav')]\n",
    "audio_samples = []\n",
    "audio_ids = []\n",
    "for audio_file in audio_files:\n",
    "    audio_id = audio_file.split('.')[0]  # Extract audio ID from the file name\n",
    "    audio_path = os.path.join(audio_folder, audio_file)\n",
    "    audio, sr = librosa.load(audio_path, sr=None)\n",
    "    audio_samples.append(audio)\n",
    "    audio_ids.append(audio_id)\n",
    "\n",
    "# Load quality ratings\n",
    "ratings_df = pd.read_csv(ratings_file, sep='\\t')\n",
    "quality_ratings = []\n",
    "for audio_id in audio_ids:\n",
    "    rating = ratings_df[ratings_df['utteranceId'] == audio_id]['choice'].values\n",
    "    if len(rating) > 0:\n",
    "        quality_ratings.append(rating[0])\n",
    "\n",
    "# Convert audio samples and quality ratings to numpy arrays\n",
    "audio_samples = np.array(audio_samples, dtype=object)\n",
    "quality_ratings = np.array(quality_ratings)\n",
    "# Subtract 1 from the quality ratings to convert them to valid class indices\n",
    "quality_ratings -= 1\n",
    "\n",
    "print(audio_samples)\n",
    "print(quality_ratings)\n",
    "\n",
    "\n",
    "# Perform train-test split\n",
    "x_train, x_val, y_train, y_val = train_test_split(audio_samples, quality_ratings, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess audio samples\n",
    "def preprocess_audio_samples(audio_samples):\n",
    "    preprocessed_samples = []\n",
    "    for audio in audio_samples:\n",
    "        # Apply audio preprocessing if needed\n",
    "        preprocessed_samples.append(audio)\n",
    "    return preprocessed_samples\n",
    "\n",
    "preprocessed_audio_samples_train = preprocess_audio_samples(x_train)\n",
    "preprocessed_audio_samples_val = preprocess_audio_samples(x_val)\n",
    "\n",
    "# Function to extract MFCC features from audio samples\n",
    "def extract_mfcc_features(audio_samples):\n",
    "    mfcc_features = []\n",
    "    for audio in audio_samples:\n",
    "        mfcc = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=num_mfcc)\n",
    "        mfcc_features.append(mfcc.T)\n",
    "    return mfcc_features\n",
    "\n",
    "# Set the parameters for MFCC feature extraction\n",
    "sample_rate = 44100  # Sample rate of your audio data\n",
    "num_mfcc = 13  # Number of MFCC coefficients to extract\n",
    "\n",
    "# Extract MFCC features for training data\n",
    "feature_representation_train = extract_mfcc_features(preprocessed_audio_samples_train)\n",
    "\n",
    "# Extract MFCC features for validation data\n",
    "feature_representation_val = extract_mfcc_features(preprocessed_audio_samples_val)\n",
    "\n",
    "# Convert the feature representation and ratings to numpy arrays\n",
    "X_train = np.array(feature_representation_train,dtype=object)\n",
    "X_train = pad_sequences(X_train, dtype='float32', padding='post')\n",
    "X_val = np.array(feature_representation_val,dtype=object)\n",
    "X_val = pad_sequences(X_val, dtype='float32', padding='post')\n",
    "\n",
    "# Determine the number of classes\n",
    "num_classes = np.max(quality_ratings) + 1\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_val)\n",
    "# One-hot encode the labels\n",
    "num_classes = 5  # Number of rating classes\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_val = to_categorical(y_val, num_classes)\n",
    "\n",
    "# # Define the model architecture\n",
    "# model = tf.keras.Sequential([\n",
    "#     layers.Input(shape=X_train[0].shape),\n",
    "#     # Add layers to your model as needed\n",
    "#     # Example:\n",
    "#     layers.Dense(64, activation='relu'),\n",
    "#     layers.Dense(5, activation='softmax')\n",
    "# ])\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "# # Train the model\n",
    "# model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "# def convert_audio_to_feature_representation(audio_samples):\n",
    "#     feature_representation = []\n",
    "#     for audio in audio_samples:\n",
    "#         # Extract MFCC features\n",
    "#         mfcc = librosa.feature.mfcc(audio, sr=sample_rate, n_mfcc=13)\n",
    "#         feature_representation.append(mfcc.T)  # Append the feature representation to the list\n",
    "#     return feature_representation\n",
    "\n",
    "\n",
    "# # Make predictions on new audio samples\n",
    "# new_audio_samples = 'audios/audios/booksent_2012_0005_164.wav'  # Provide new audio samples to predict on\n",
    "# preprocessed_new_audio_samples = preprocess_audio_samples(new_audio_samples)\n",
    "# new_feature_representation = convert_audio_to_feature_representation(preprocessed_new_audio_samples)\n",
    "# X_new = np.array(new_feature_representation)\n",
    "# predictions = model.predict(X_new)\n",
    "# print(predictions)\n",
    "\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Reshape((num_classes,), input_shape=(num_classes,)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "\n",
    "def convert_audio_to_feature_representation(audio_samples):\n",
    "    feature_representation = []\n",
    "    for audio in audio_samples:\n",
    "        # Extract MFCC features\n",
    "        mfcc = librosa.feature.mfcc(audio, sr=sample_rate, n_mfcc=13)\n",
    "        feature_representation.append(mfcc.T)  # Append the feature representation to the list\n",
    "    return feature_representation\n",
    "\n",
    "\n",
    "# Make predictions on new audio samples\n",
    "new_audio_samples = 'audios/audios/booksent_2012_0005_164.wav'  # Provide new audio samples to predict on\n",
    "preprocessed_new_audio_samples = preprocess_audio_samples(new_audio_samples)\n",
    "new_feature_representation = convert_audio_to_feature_representation(preprocessed_new_audio_samples)\n",
    "X_new = np.array(new_feature_representation)\n",
    "predictions = model.predict(X_new)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\neera\\AppData\\Local\\Temp\\ipykernel_18304\\3872023439.py:8: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  data = librosa.load('somos.zip', sr=16000)\n",
      "C:\\Users\\neera\\AppData\\Roaming\\Python\\Python311\\site-packages\\librosa\\core\\audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'somos.zip'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLibsndfileError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\librosa\\core\\audio.py:176\u001b[0m, in \u001b[0;36mload\u001b[1;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 176\u001b[0m     y, sr_native \u001b[39m=\u001b[39m __soundfile_load(path, offset, duration, dtype)\n\u001b[0;32m    178\u001b[0m \u001b[39mexcept\u001b[39;00m sf\u001b[39m.\u001b[39mSoundFileRuntimeError \u001b[39mas\u001b[39;00m exc:\n\u001b[0;32m    179\u001b[0m     \u001b[39m# If soundfile failed, try audioread instead\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\librosa\\core\\audio.py:209\u001b[0m, in \u001b[0;36m__soundfile_load\u001b[1;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[39m# Otherwise, create the soundfile object\u001b[39;00m\n\u001b[1;32m--> 209\u001b[0m     context \u001b[39m=\u001b[39m sf\u001b[39m.\u001b[39;49mSoundFile(path)\n\u001b[0;32m    211\u001b[0m \u001b[39mwith\u001b[39;00m context \u001b[39mas\u001b[39;00m sf_desc:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\soundfile.py:658\u001b[0m, in \u001b[0;36mSoundFile.__init__\u001b[1;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd)\u001b[0m\n\u001b[0;32m    656\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info \u001b[39m=\u001b[39m _create_info_struct(file, mode, samplerate, channels,\n\u001b[0;32m    657\u001b[0m                                  \u001b[39mformat\u001b[39m, subtype, endian)\n\u001b[1;32m--> 658\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_file \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_open(file, mode_int, closefd)\n\u001b[0;32m    659\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mset\u001b[39m(mode)\u001b[39m.\u001b[39missuperset(\u001b[39m'\u001b[39m\u001b[39mr+\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseekable():\n\u001b[0;32m    660\u001b[0m     \u001b[39m# Move write position to 0 (like in Python file objects)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\soundfile.py:1216\u001b[0m, in \u001b[0;36mSoundFile._open\u001b[1;34m(self, file, mode_int, closefd)\u001b[0m\n\u001b[0;32m   1215\u001b[0m     err \u001b[39m=\u001b[39m _snd\u001b[39m.\u001b[39msf_error(file_ptr)\n\u001b[1;32m-> 1216\u001b[0m     \u001b[39mraise\u001b[39;00m LibsndfileError(err, prefix\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError opening \u001b[39m\u001b[39m{0!r}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname))\n\u001b[0;32m   1217\u001b[0m \u001b[39mif\u001b[39;00m mode_int \u001b[39m==\u001b[39m _snd\u001b[39m.\u001b[39mSFM_WRITE:\n\u001b[0;32m   1218\u001b[0m     \u001b[39m# Due to a bug in libsndfile version <= 1.0.25, frames != 0\u001b[39;00m\n\u001b[0;32m   1219\u001b[0m     \u001b[39m# when opening a named pipe in SFM_WRITE mode.\u001b[39;00m\n\u001b[0;32m   1220\u001b[0m     \u001b[39m# See http://github.com/erikd/libsndfile/issues/77.\u001b[39;00m\n",
      "\u001b[1;31mLibsndfileError\u001b[0m: Error opening 'somos.zip': System error.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcallbacks\u001b[39;00m \u001b[39mimport\u001b[39;00m ModelCheckpoint\n\u001b[0;32m      7\u001b[0m \u001b[39m# Load the dataset\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m data \u001b[39m=\u001b[39m librosa\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39msomos.zip\u001b[39;49m\u001b[39m'\u001b[39;49m, sr\u001b[39m=\u001b[39;49m\u001b[39m16000\u001b[39;49m)\n\u001b[0;32m     10\u001b[0m \u001b[39m# Extract the features\u001b[39;00m\n\u001b[0;32m     11\u001b[0m features \u001b[39m=\u001b[39m librosa\u001b[39m.\u001b[39mfeature\u001b[39m.\u001b[39mmfcc(data, n_mfcc\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m, hop_length\u001b[39m=\u001b[39m\u001b[39m160\u001b[39m, win_length\u001b[39m=\u001b[39m\u001b[39m400\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\librosa\\core\\audio.py:184\u001b[0m, in \u001b[0;36mload\u001b[1;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(path, (\u001b[39mstr\u001b[39m, pathlib\u001b[39m.\u001b[39mPurePath)):\n\u001b[0;32m    181\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    182\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPySoundFile failed. Trying audioread instead.\u001b[39m\u001b[39m\"\u001b[39m, stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m\n\u001b[0;32m    183\u001b[0m     )\n\u001b[1;32m--> 184\u001b[0m     y, sr_native \u001b[39m=\u001b[39m __audioread_load(path, offset, duration, dtype)\n\u001b[0;32m    185\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    186\u001b[0m     \u001b[39mraise\u001b[39;00m exc\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m kwsyntax:\n\u001b[0;32m    231\u001b[0m     args, kw \u001b[39m=\u001b[39m fix(args, kw, sig)\n\u001b[1;32m--> 232\u001b[0m \u001b[39mreturn\u001b[39;00m caller(func, \u001b[39m*\u001b[39;49m(extras \u001b[39m+\u001b[39;49m args), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\librosa\\util\\decorators.py:60\u001b[0m, in \u001b[0;36mdeprecated.<locals>.__wrapper\u001b[1;34m(func, *args, **kwargs)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Warn the user, and then proceed.\"\"\"\u001b[39;00m\n\u001b[0;32m     52\u001b[0m warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m     53\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m{:s}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{:s}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39mDeprecated as of librosa version \u001b[39m\u001b[39m{:s}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     54\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39mIt will be removed in librosa version \u001b[39m\u001b[39m{:s}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     58\u001b[0m     stacklevel\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m,  \u001b[39m# Would be 2, but the decorator adds a level\u001b[39;00m\n\u001b[0;32m     59\u001b[0m )\n\u001b[1;32m---> 60\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\librosa\\core\\audio.py:241\u001b[0m, in \u001b[0;36m__audioread_load\u001b[1;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[0;32m    238\u001b[0m     reader \u001b[39m=\u001b[39m path\n\u001b[0;32m    239\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    240\u001b[0m     \u001b[39m# If the input was not an audioread object, try to open it\u001b[39;00m\n\u001b[1;32m--> 241\u001b[0m     reader \u001b[39m=\u001b[39m audioread\u001b[39m.\u001b[39;49maudio_open(path)\n\u001b[0;32m    243\u001b[0m \u001b[39mwith\u001b[39;00m reader \u001b[39mas\u001b[39;00m input_file:\n\u001b[0;32m    244\u001b[0m     sr_native \u001b[39m=\u001b[39m input_file\u001b[39m.\u001b[39msamplerate\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\audioread\\__init__.py:127\u001b[0m, in \u001b[0;36maudio_open\u001b[1;34m(path, backends)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[39mfor\u001b[39;00m BackendClass \u001b[39min\u001b[39;00m backends:\n\u001b[0;32m    126\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 127\u001b[0m         \u001b[39mreturn\u001b[39;00m BackendClass(path)\n\u001b[0;32m    128\u001b[0m     \u001b[39mexcept\u001b[39;00m DecodeError:\n\u001b[0;32m    129\u001b[0m         \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\audioread\\rawread.py:59\u001b[0m, in \u001b[0;36mRawAudioFile.__init__\u001b[1;34m(self, filename)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, filename):\n\u001b[1;32m---> 59\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fh \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(filename, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     61\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_file \u001b[39m=\u001b[39m aifc\u001b[39m.\u001b[39mopen(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fh)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'somos.zip'"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Load the dataset\n",
    "data = librosa.load('somos.zip', sr=16000)\n",
    "\n",
    "# Extract the features\n",
    "features = librosa.feature.mfcc(data, n_mfcc=20, hop_length=160, win_length=400)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, data, test_size=0.25)\n",
    "\n",
    "# Normalize the data\n",
    "X_train = np.asarray(X_train)\n",
    "X_test = np.asarray(X_test)\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(X_train.shape[1],)))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "# Save the model\n",
    "model_path = 'model.h5'\n",
    "model.save(model_path)\n",
    "\n",
    "# Make predictions for new audios\n",
    "def predict(audio):\n",
    "    features = librosa.feature.mfcc(audio, n_mfcc=20, hop_length=160, win_length=400)\n",
    "    features = np.asarray(features)\n",
    "    prediction = model.predict(features)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'My father in law was diagnosed with pancreatic cancer when our daughter was  nine  months old. He passed  nine  months later when she was  one  eight  months.\\t\\t\\t\\t\\t\\nSad-312'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 69\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[0;32m     68\u001b[0m model \u001b[39m=\u001b[39m LogisticRegression()\n\u001b[1;32m---> 69\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[0;32m     71\u001b[0m \u001b[39m# Load and process the audio file\u001b[39;00m\n\u001b[0;32m     72\u001b[0m audio_file \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39moutput.mp3\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:1196\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1193\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1194\u001b[0m     _dtype \u001b[39m=\u001b[39m [np\u001b[39m.\u001b[39mfloat64, np\u001b[39m.\u001b[39mfloat32]\n\u001b[1;32m-> 1196\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m   1197\u001b[0m     X,\n\u001b[0;32m   1198\u001b[0m     y,\n\u001b[0;32m   1199\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1200\u001b[0m     dtype\u001b[39m=\u001b[39;49m_dtype,\n\u001b[0;32m   1201\u001b[0m     order\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mC\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1202\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49msolver \u001b[39mnot\u001b[39;49;00m \u001b[39min\u001b[39;49;00m [\u001b[39m\"\u001b[39;49m\u001b[39mliblinear\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39msag\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39msaga\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m   1203\u001b[0m )\n\u001b[0;32m   1204\u001b[0m check_classification_targets(y)\n\u001b[0;32m   1205\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_ \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(y)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:584\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    582\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[0;32m    583\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 584\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[0;32m    585\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    587\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py:1106\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1101\u001b[0m         estimator_name \u001b[39m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1102\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1103\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m requires y to be passed, but the target y is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1104\u001b[0m     )\n\u001b[1;32m-> 1106\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m   1107\u001b[0m     X,\n\u001b[0;32m   1108\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[0;32m   1109\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49maccept_large_sparse,\n\u001b[0;32m   1110\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m   1111\u001b[0m     order\u001b[39m=\u001b[39;49morder,\n\u001b[0;32m   1112\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m   1113\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[0;32m   1114\u001b[0m     ensure_2d\u001b[39m=\u001b[39;49mensure_2d,\n\u001b[0;32m   1115\u001b[0m     allow_nd\u001b[39m=\u001b[39;49mallow_nd,\n\u001b[0;32m   1116\u001b[0m     ensure_min_samples\u001b[39m=\u001b[39;49mensure_min_samples,\n\u001b[0;32m   1117\u001b[0m     ensure_min_features\u001b[39m=\u001b[39;49mensure_min_features,\n\u001b[0;32m   1118\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[0;32m   1119\u001b[0m     input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1120\u001b[0m )\n\u001b[0;32m   1122\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[0;32m   1124\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py:879\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    877\u001b[0m         array \u001b[39m=\u001b[39m xp\u001b[39m.\u001b[39mastype(array, dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    878\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 879\u001b[0m         array \u001b[39m=\u001b[39m _asarray_with_order(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype, xp\u001b[39m=\u001b[39;49mxp)\n\u001b[0;32m    880\u001b[0m \u001b[39mexcept\u001b[39;00m ComplexWarning \u001b[39mas\u001b[39;00m complex_warning:\n\u001b[0;32m    881\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    882\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mComplex data not supported\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[0;32m    883\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\_array_api.py:185\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    182\u001b[0m     xp, _ \u001b[39m=\u001b[39m get_namespace(array)\n\u001b[0;32m    183\u001b[0m \u001b[39mif\u001b[39;00m xp\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39min\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mnumpy\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mnumpy.array_api\u001b[39m\u001b[39m\"\u001b[39m}:\n\u001b[0;32m    184\u001b[0m     \u001b[39m# Use NumPy API to support order\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39masarray(array, order\u001b[39m=\u001b[39morder, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[0;32m    186\u001b[0m     \u001b[39mreturn\u001b[39;00m xp\u001b[39m.\u001b[39masarray(array, copy\u001b[39m=\u001b[39mcopy)\n\u001b[0;32m    187\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'My father in law was diagnosed with pancreatic cancer when our daughter was  nine  months old. He passed  nine  months later when she was  one  eight  months.\\t\\t\\t\\t\\t\\nSad-312'"
     ]
    }
   ],
   "source": [
    "\"\"\" #to identify emotions\n",
    "\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "df = pd.read_csv('E:\\Samsung-Test\\emotions.csv')\n",
    "\n",
    "# Extract input features (X) and labels (y)\n",
    "X = df['Emotion'].values\n",
    "y = df['Text'].values\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "X = label_encoder.fit_transform(X)\n",
    "\n",
    " # Load the dataset\n",
    "# data = np.load('E:\\Samsung-Test\\emotions.npz')\n",
    "# X = data['X']\n",
    "# y = data['y'] \n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "# Train the model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions for new values\n",
    "new_input = librosa.load('output.mp3')\n",
    "new_features = librosa.feature.mfcc(new_input)\n",
    "new_prediction = model.predict(new_features)\n",
    "\n",
    "\n",
    "# Decode the predicted labels back to their original string values\n",
    "predicted_labels = label_encoder.inverse_transform(new_prediction)\n",
    "\n",
    "# Print the prediction\n",
    "print(predicted_labels)\n",
    " \"\"\"\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "# Read the CSV file containing the training data\n",
    "df = pd.read_csv('E:\\Samsung-Test\\emotions.csv')\n",
    "\n",
    "# Extract input features (X) and labels (y)\n",
    "X = df['Text'].values\n",
    "y = df['Emotion'].values\n",
    "\n",
    "# Encode string labels to numeric values\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "# Train the model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Load and process the audio file\n",
    "audio_file = 'output.mp3'\n",
    "audio_data, sr = librosa.load(audio_file, duration=3)  # Load only the first 3 seconds\n",
    "audio_features = librosa.feature.mfcc(y=audio_data, sr=sr)\n",
    "\n",
    "# Reshape the audio features to match the expected input shape of the model\n",
    "audio_features = audio_features.reshape(1, -1)\n",
    "\n",
    "# Make predictions on the audio features\n",
    "predicted_label = label_encoder.inverse_transform(model.predict(audio_features))\n",
    "\n",
    "# Print the predicted emotion\n",
    "print(predicted_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" import csv\n",
    "\n",
    "# Open the text file\n",
    "with open('E:\\Samsung-Test\\B.txt', 'r') as file:\n",
    "    # Read the file content\n",
    "    content = file.read()\n",
    "    \n",
    "    # Split the content based on \"|\"\n",
    "    split_content = content.split(\"|\")\n",
    "    \n",
    "    # Save the split content to a CSV file\n",
    "    with open('E:\\Samsung-Test\\output2.csv','a',  newline='') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        for item in split_content:\n",
    "            writer.writerow([item.strip()])  # Optional: Use strip() to remove leading/trailing whitespaces\n",
    "\n",
    "\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Anger' 'Happy' 'Base' 'Base' 'Anger' 'Calm' 'Happy' 'Surprise' 'Calm'\n",
      " 'Anger' 'Fear' 'Calm' 'Surprise' 'Surprise' 'Surprise' 'Surprise' 'Sad'\n",
      " 'Calm' 'Excited' 'Surprise' 'Anger' 'Calm' 'Surprise' 'Apologetic' 'Base'\n",
      " 'Apologetic' 'Base' 'Apologetic' 'Anger' 'Surprise' 'Sad' 'Calm' 'Anger'\n",
      " 'Surprise' 'Anger' 'Apologetic' 'Excited' 'Base' 'Excited' 'Sad'\n",
      " 'Excited' 'Sad' 'Surprise' 'Calm' 'Surprise' 'Sad' 'Surprise' 'Anger'\n",
      " 'Base' 'Apologetic' 'Happy' 'Anger' 'Calm' 'Base' 'Anger' 'Surprise'\n",
      " 'Happy' 'Fear' 'Anger' 'Happy' 'Excited' 'Anger' 'Surprise' 'Excited'\n",
      " 'Surprise' 'Calm' 'Fear' 'Fear' 'Base' 'Anger' 'Sad' 'Base' 'Happy'\n",
      " 'Excited' 'Apologetic' 'Surprise' 'Surprise' 'Anger' 'Base' 'Calm' 'Sad'\n",
      " 'Sad' 'Excited' 'Fear' 'Surprise' 'Excited' 'Surprise' 'Sad' 'Fear'\n",
      " 'Anger' 'Fear' 'Anger' 'Excited' 'Excited' 'Excited' 'Calm' 'Excited'\n",
      " 'Anger' 'Apologetic' 'Anger' 'Anger' 'Anger' 'Happy' 'Apologetic' 'Anger'\n",
      " 'Fear' 'Excited' 'Happy' 'Calm' 'Excited' 'Anger' 'Sad' 'Calm' 'Calm'\n",
      " 'Anger' 'Happy' 'Excited' 'Happy' 'Calm' 'Calm' 'Calm' 'Apologetic'\n",
      " 'Happy' 'Anger' 'Calm' 'Calm' 'Base' 'Excited' 'Anger' 'Happy' 'Base'\n",
      " 'Surprise' 'Anger' 'Apologetic' 'Anger' 'Calm' 'Anger' 'Calm' 'Anger'\n",
      " 'Base' 'Anger' 'Anger' 'Sad' 'Apologetic' 'Apologetic' 'Fear' 'Excited'\n",
      " 'Anger' 'Base' 'Surprise' 'Anger' 'Base' 'Calm' 'Anger' 'Surprise'\n",
      " 'Happy' 'Apologetic' 'Calm' 'Sad' 'Calm' 'Surprise' 'Happy' 'Anger'\n",
      " 'Surprise' 'Excited' 'Surprise' 'Calm' 'Sad' 'Happy' 'Calm' 'Apologetic'\n",
      " 'Happy' 'Anger' 'Calm' 'Anger' 'Fear' 'Anger' 'Surprise' 'Sad' 'Happy'\n",
      " 'Anger' 'Fear' 'Anger' 'Sad' 'Excited' 'Happy' 'Happy' 'Excited' 'Base'\n",
      " 'Apologetic' 'Anger' 'Happy' 'Fear' 'Excited' 'Excited' 'Anger'\n",
      " 'Surprise' 'Anger' 'Base' 'Base' 'Calm' 'Excited' 'Sad' 'Base' 'Fear'\n",
      " 'Apologetic' 'Excited' 'Surprise' 'Sad' 'Anger' 'Fear' 'Sad' 'Anger'\n",
      " 'Anger' 'Surprise' 'Happy' 'Apologetic' 'Sad' 'Anger' 'Surprise' 'Base'\n",
      " 'Calm' 'Anger' 'Apologetic' 'Apologetic' 'Happy' 'Fear' 'Surprise' 'Fear'\n",
      " 'Fear' 'Excited' 'Fear' 'Fear' 'Fear' 'Surprise' 'Base' 'Happy' 'Excited'\n",
      " 'Sad' 'Apologetic' 'Anger' 'Surprise' 'Anger' 'Fear' 'Sad' 'Base'\n",
      " 'Excited' 'Fear' 'Sad' 'Apologetic' 'Anger' 'Calm' 'Sad' 'Anger'\n",
      " 'Surprise' 'Base' 'Base' 'Base' 'Anger' 'Calm' 'Calm' 'Apologetic'\n",
      " 'Apologetic' 'Apologetic' 'Sad' 'Base' 'Anger' 'Base' 'Calm' 'Excited'\n",
      " 'Sad' 'Sad' 'Sad' 'Base' 'Calm' 'Base' 'Fear' 'Sad' 'Surprise' 'Base'\n",
      " 'Surprise' 'Happy' 'Sad' 'Base' 'Base' 'Base' 'Base' 'Anger' 'Base'\n",
      " 'Happy' 'Apologetic' 'Calm' 'Excited' 'Excited' 'Fear' 'Apologetic'\n",
      " 'Calm' 'Base' 'Base' 'Excited' 'Surprise' 'Surprise' 'Sad' 'Base'\n",
      " 'Excited' 'Sad' 'Surprise' 'Surprise' 'Base' 'Apologetic' 'Fear' 'Anger'\n",
      " 'Happy' 'Anger' 'Calm' 'Anger' 'Fear' 'Apologetic' 'Excited' 'Apologetic'\n",
      " 'Happy' 'Calm' 'Apologetic' 'Happy' 'Apologetic' 'Base' 'Anger' 'Excited'\n",
      " 'Happy' 'Happy' 'Apologetic' 'Happy' 'Base' 'Surprise' 'Calm' 'Surprise'\n",
      " 'Base' 'Apologetic' 'Happy' 'Anger' 'Apologetic' 'Apologetic' 'Calm'\n",
      " 'Anger' 'Happy' 'Happy' 'Happy' 'Happy' 'Happy' 'Happy' 'Fear' 'Surprise'\n",
      " 'Anger' 'Apologetic' 'Calm' 'Calm' 'Anger' 'Sad' 'Base' 'Base' 'Sad'\n",
      " 'Apologetic' 'Fear' 'Calm' 'Calm' 'Sad' 'Excited' 'Sad' 'Base' 'Fear'\n",
      " 'Apologetic' 'Fear' 'Base' 'Anger' 'Sad' 'Sad' 'Happy' 'Calm' 'Calm'\n",
      " 'Fear' 'Surprise' 'Apologetic' 'Anger' 'Happy' 'Sad' 'Fear' 'Calm'\n",
      " 'Happy' 'Excited' 'Excited' 'Excited' 'Sad' 'Calm' 'Surprise'\n",
      " 'Apologetic' 'Sad' 'Excited' 'Base' 'Fear' 'Surprise' 'Surprise'\n",
      " 'Apologetic' 'Surprise' 'Calm' 'Calm' 'Anger' 'Fear' 'Excited'\n",
      " 'Apologetic' 'Excited' 'Excited' 'Base' 'Happy' 'Sad' 'Fear' 'Base'\n",
      " 'Surprise' 'Happy' 'Calm' 'Sad' 'Apologetic' 'Anger' 'Fear' 'Excited'\n",
      " 'Excited' 'Fear' 'Base' 'Happy' 'Surprise' 'Anger' 'Fear' 'Happy' 'Calm'\n",
      " 'Apologetic' 'Apologetic' 'Happy' 'Base' 'Happy' 'Calm' 'Surprise' 'Sad'\n",
      " 'Apologetic' 'Sad' 'Surprise' 'Apologetic' 'Apologetic' 'Anger' 'Calm'\n",
      " 'Calm' 'Sad' 'Base' 'Happy' 'Happy' 'Excited' 'Sad' 'Happy' 'Surprise'\n",
      " 'Calm' 'Calm' 'Anger' 'Calm' 'Apologetic' 'Fear' 'Base' 'Anger' 'Fear'\n",
      " 'Anger' 'Anger' 'Happy' 'Calm' 'Anger' 'Apologetic' 'Excited' 'Calm'\n",
      " 'Surprise' 'Calm' 'Calm' 'Calm' 'Excited' 'Excited' 'Happy' 'Base'\n",
      " 'Anger' 'Sad' 'Anger' 'Happy' 'Excited' 'Sad' 'Excited' 'Surprise' 'Calm'\n",
      " 'Excited' 'Surprise' 'Happy' 'Apologetic' 'Fear' 'Calm' 'Surprise' 'Base'\n",
      " 'Base' 'Base' 'Sad' 'Excited' 'Fear' 'Apologetic' 'Happy' 'Surprise'\n",
      " 'Excited' 'Fear' 'Excited' 'Excited' 'Calm' 'Base' 'Sad' 'Fear' 'Happy'\n",
      " 'Base' 'Happy' 'Excited' 'Happy' 'Base' 'Happy' 'Excited' 'Base' 'Happy'\n",
      " 'Excited' 'Happy' 'Happy' 'Sad' 'Surprise' 'Apologetic' 'Apologetic'\n",
      " 'Calm' 'Base' 'Excited' 'Happy' 'Calm' 'Apologetic' 'Sad' 'Anger' 'Happy'\n",
      " 'Apologetic' 'Anger' 'Excited' 'Excited' 'Excited' 'Surprise' 'Happy'\n",
      " 'Calm' 'Calm' 'Excited' 'Apologetic' 'Base' 'Base' 'Calm' 'Base' 'Calm'\n",
      " 'Apologetic' 'Anger' 'Sad' 'Excited' 'Fear' 'Excited' 'Apologetic' 'Fear'\n",
      " 'Surprise' 'Apologetic' 'Fear' 'Happy' 'Sad' 'Happy' 'Sad' 'Happy'\n",
      " 'Anger' 'Sad' 'Excited' 'Apologetic' 'Fear' 'Surprise' 'Anger' 'Excited'\n",
      " 'Apologetic' 'Anger' 'Anger' 'Excited' 'Happy' 'Calm' 'Anger' 'Happy'\n",
      " 'Excited' 'Apologetic' 'Calm' 'Base' 'Apologetic' 'Fear' 'Fear' 'Excited'\n",
      " 'Fear' 'Base' 'Base' 'Excited' 'Happy' 'Calm' 'Happy' 'Anger' 'Anger'\n",
      " 'Fear' 'Happy' 'Fear' 'Happy' 'Anger' 'Fear' 'Anger' 'Sad' 'Anger' 'Fear'\n",
      " 'Base' 'Base' 'Apologetic' 'Excited' 'Apologetic' 'Anger' 'Anger' 'Sad'\n",
      " 'Calm' 'Excited' 'Apologetic' 'Anger' 'Sad' 'Calm' 'Base' 'Surprise'\n",
      " 'Calm' 'Apologetic' 'Base' 'Sad' 'Base' 'Happy' 'Surprise' 'Happy' 'Base'\n",
      " 'Happy' 'Base' 'Apologetic' 'Excited' 'Happy' 'Calm' 'Sad' 'Excited'\n",
      " 'Excited' 'Calm' 'Anger' 'Apologetic' 'Calm' 'Happy' 'Sad' 'Calm' 'Happy'\n",
      " 'Excited' 'Surprise' 'Fear' 'Happy' 'Base' 'Fear' 'Anger' 'Apologetic'\n",
      " 'Apologetic' 'Sad' 'Sad' 'Base' 'Anger' 'Base' 'Fear' 'Sad' 'Apologetic'\n",
      " 'Sad' 'Sad' 'Surprise' 'Happy' 'Calm' 'Calm' 'Anger' 'Sad' 'Happy' 'Base'\n",
      " 'Excited' 'Happy' 'Sad' 'Surprise' 'Base' 'Calm' 'Happy' 'Anger' 'Anger'\n",
      " 'Happy' 'Calm' 'Happy' 'Happy' 'Happy' 'Anger' 'Excited' 'Surprise'\n",
      " 'Base' 'Apologetic' 'Surprise' 'Sad' 'Apologetic' 'Fear' 'Anger'\n",
      " 'Excited' 'Surprise' 'Apologetic' 'Anger' 'Anger' 'Base' 'Anger' 'Fear'\n",
      " 'Calm' 'Happy' 'Happy' 'Excited' 'Apologetic' 'Surprise' 'Base' 'Happy'\n",
      " 'Base' 'Excited' 'Sad' 'Excited' 'Anger' 'Sad' 'Calm' 'Calm' 'Apologetic'\n",
      " 'Happy' 'Calm' 'Surprise' 'Base' 'Anger' 'Excited' 'Excited' 'Anger'\n",
      " 'Anger' 'Excited' 'Surprise' 'Anger' 'Excited' 'Sad' 'Excited' 'Base'\n",
      " 'Excited' 'Excited' 'Surprise' 'Sad' 'Excited' 'Happy' 'Anger' 'Happy'\n",
      " 'Surprise' 'Base' 'Fear' 'Happy' 'Surprise' 'Sad' 'Surprise' 'Base'\n",
      " 'Anger' 'Calm' 'Excited' 'Base' 'Happy' 'Excited' 'Surprise' 'Calm'\n",
      " 'Base' 'Surprise' 'Surprise' 'Base' 'Happy' 'Sad' 'Excited' 'Happy'\n",
      " 'Excited' 'Apologetic' 'Fear' 'Base' 'Surprise' 'Excited' 'Calm' 'Calm'\n",
      " 'Excited' 'Calm' 'Anger']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('emotions.csv')\n",
    "\n",
    "# Replace missing values with an empty string\n",
    "df['Text'].fillna('', inplace=True)\n",
    "\n",
    "# Extract input features (X) and labels (y)\n",
    "X = df['Text'].values\n",
    "y = df['Emotion'].values\n",
    "\n",
    "# Convert emotions to numerical labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Vectorize the text data using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "# Train the model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predicted_labels = label_encoder.inverse_transform(model.predict(X_test))\n",
    "\n",
    "# Print the predictions\n",
    "print(predicted_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Base']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import librosa\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('emotions.csv')\n",
    "\n",
    "# Replace missing values with an empty string\n",
    "df['Text'].fillna('', inplace=True)\n",
    "\n",
    "# Extract input features (X) and labels (y)\n",
    "X = df['Text'].values\n",
    "y = df['Emotion'].values\n",
    "\n",
    "# Convert emotions to numerical labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Vectorize the text data using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "# Train the model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Load and process the audio file\n",
    "audio_file = 'output.mp3'\n",
    "audio_data, sr = librosa.load(audio_file, duration=3)  # Load only the first 3 seconds\n",
    "audio_text = librosa.feature.mfcc(y=audio_data, sr=sr)\n",
    "\n",
    "# Convert the audio text array to string representation\n",
    "audio_text_str = ' '.join(map(str, audio_text))\n",
    "\n",
    "# Vectorize the audio text using the same TF-IDF vectorizer\n",
    "audio_text_vectorized = vectorizer.transform([audio_text_str])\n",
    "\n",
    "# Make predictions on the audio text\n",
    "predicted_label = label_encoder.inverse_transform(model.predict(audio_text_vectorized))\n",
    "\n",
    "# Print the prediction\n",
    "print(predicted_label)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
